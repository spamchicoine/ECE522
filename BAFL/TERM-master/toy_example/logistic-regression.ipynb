{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtCH90yeEser"
   },
   "outputs": [],
   "source": [
    "# code adapted from: https://github.com/martinpella/logistic-reg/blob/master/logistic_reg.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKmzsNAqEsev"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, alpha=0, lr=0.01, num_iter=10000, fit_intercept=True):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.alpha=alpha\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((X, intercept), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        obj_values = []\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            loss = np.log(1+np.exp(-z)) + np.multiply(1-y, z)\n",
    "            \n",
    "            gradient = np.dot(X.T, np.multiply(np.exp(loss * self.alpha), h - y)) / y.size\n",
    "            ZZ = np.mean(np.exp(self.alpha * loss))\n",
    "            \n",
    "            self.theta -= self.lr /ZZ * gradient\n",
    "            \n",
    "            \n",
    "            obj_values.append((1/self.alpha) * np.log(np.mean(np.exp(self.alpha * loss))))\n",
    "            \n",
    "\n",
    "        return obj_values, loss\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21T8yzqAEse9"
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "w = np.random.random_sample(3)\n",
    "print(w)\n",
    "\n",
    "w, b = w[:2], w[-1]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "XX = np.random.normal([3, 1], [1, 1], (2000, 2))\n",
    "XX= np.append(XX, np.random.normal([-5, -1], [2, 2], (2000, 2)), axis=0)\n",
    "\n",
    "positive = 3\n",
    "negative = 50\n",
    "hard_positive = 3\n",
    "hard_negative = 5\n",
    "\n",
    "for i in range(len(XX)):\n",
    "    t = np.dot(w, XX[i]) + b\n",
    "    if t > 0.2 and t<1.5 and hard_positive > 0:\n",
    "        y.append(1)\n",
    "        X.append(XX[i])\n",
    "        hard_positive -= 1\n",
    "    elif t < -0.2 and t>-.5 and hard_negative > 0:\n",
    "        y.append(0)\n",
    "        X.append(XX[i])\n",
    "        hard_negative -= 1\n",
    "    elif t > 1.5 and positive > 0:\n",
    "        y.append(1)\n",
    "        X.append(XX[i])\n",
    "        positive -= 1\n",
    "    elif t < -.5 and negative > 0:\n",
    "        y.append(0)\n",
    "        X.append(XX[i])\n",
    "        negative -= 1\n",
    "\n",
    "X.append(np.array([-8, 0]))\n",
    "y.append(1)\n",
    "X, y = np.array(X), np.array(y)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l6K857O-Ese_"
   },
   "outputs": [],
   "source": [
    "thetas = []\n",
    "\n",
    "\n",
    "idx  = 0\n",
    "for a in -1*np.flip(np.logspace(0, 1, 10)):\n",
    "    model = LogisticRegression(alpha=a, lr=0.01, num_iter=10000)\n",
    "    _, loss = model.fit(X, y)\n",
    "    print(\"idx: {}, max loss: {}, variance: {}\".format(idx, max(loss), np.var(loss)))\n",
    "    preds = model.predict(X)\n",
    "    # accuracy\n",
    "    accu = (preds == y).mean()\n",
    "    print(\"t= {}, slop= {}\".format(a, -1*model.theta[0]/model.theta[1]))\n",
    "    idx += 1\n",
    "    thetas.append([model.theta[0], model.theta[1], model.theta[2]])\n",
    "    \n",
    "for a in -1*np.flip(np.logspace(-1, 0, 25)):\n",
    "    model = LogisticRegression(alpha=a, lr=0.01, num_iter=10000)\n",
    "    _, loss = model.fit(X, y)\n",
    "    print(\"idx: {}, max loss: {}, variance: {}\".format(idx, max(loss), np.var(loss)))\n",
    "    preds = model.predict(X)\n",
    "    # accuracy\n",
    "    accu = (preds == y).mean()\n",
    "    print(\"t= {}, slop= {}\".format(a, -1*model.theta[0]/model.theta[1]))\n",
    "    idx += 1\n",
    "    thetas.append([model.theta[0], model.theta[1], model.theta[2]])\n",
    "    \n",
    "for a in -1*np.flip(np.logspace(-2, -1, 25)):\n",
    "    model = LogisticRegression(alpha=a, lr=0.1, num_iter=10000)\n",
    "    _, loss = model.fit(X, y)\n",
    "    print(\"idx: {}, max loss: {}, variance: {}\".format(idx, max(loss), np.var(loss)))\n",
    "    preds = model.predict(X)\n",
    "    # accuracy\n",
    "    accu = (preds == y).mean()\n",
    "    print(\"t= {}, slop= {}\".format(a, -1*model.theta[0]/model.theta[1]))\n",
    "    idx += 1\n",
    "    thetas.append([model.theta[0], model.theta[1], model.theta[2]])\n",
    "\n",
    "for a in np.logspace(-2, 0.1, 35):\n",
    "    model = LogisticRegression(alpha=a, lr=0.1, num_iter=10000)\n",
    "    _, loss = model.fit(X, y)\n",
    "    print(\"max loss: {}, variance: {}\".format(max(loss), np.var(loss)))\n",
    "    preds = model.predict(X)\n",
    "    # accuracy\n",
    "    accu = (preds == y).mean()\n",
    "    print(\"t= {}, slop= {}\".format(a, -1*model.theta[0]/model.theta[1]))\n",
    "    \n",
    "    thetas.append([model.theta[0], model.theta[1], model.theta[2]])\n",
    "    \n",
    "for a in np.logspace(0.1, 0.8, 25):\n",
    "    model = LogisticRegression(alpha=a, lr=0.001, num_iter=30000)\n",
    "    _, loss = model.fit(X, y)\n",
    "    print(\"max loss: {}, variance: {}\".format(max(loss), np.var(loss)))\n",
    "    preds = model.predict(X)\n",
    "    # accuracy\n",
    "    accu = (preds == y).mean()\n",
    "    print(\"t= {}, slop= {}\".format(a, -1*model.theta[0]/model.theta[1]))\n",
    "    \n",
    "    thetas.append([model.theta[0], model.theta[1], model.theta[2]])\n",
    "   \n",
    "    \n",
    "thetas.append([model.theta[0], model.theta[1], model.theta[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(w1, w2, b, label_, c=None, zorder=1):\n",
    "    \"\"\"Plot a line from slope and intercept\"\"\"\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(range(-10, 5))\n",
    "    y_vals = -b/w2 - (w1/w2) * x_vals\n",
    "    print(-1*w1/w2)\n",
    "    plt.plot(x_vals, y_vals, label=label_, color=c, zorder=1,  linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lighten_color(color, amount=0.5):\n",
    "    \"\"\"\n",
    "    Lightens the given color by multiplying (1-luminosity) by the given amount.\n",
    "    Input can be matplotlib color string, hex string, or RGB tuple.\n",
    "\n",
    "    Examples:\n",
    "    >> lighten_color('g', 0.3)\n",
    "    >> lighten_color('#F034A3', 0.6)\n",
    "    >> lighten_color((.3,.55,.1), 0.5)\n",
    "    \"\"\"\n",
    "    import matplotlib.colors as mc\n",
    "    import colorsys\n",
    "    try:\n",
    "        c = mc.cnames[color]\n",
    "    except:\n",
    "        c = color\n",
    "    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n",
    "    return colorsys.hls_to_rgb(c[0], 1 - amount * (1 - c[1]), c[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "\n",
    "\n",
    "print(len(thetas))\n",
    "tot = len(thetas)\n",
    "colors_positive = pl.cm.Reds(np.linspace(-0.4, 1, int(tot/2)))\n",
    "colors_negative = pl.cm.Blues(np.linspace(-0.5, 0.9, int(tot/2)))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 3.5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "for i in reversed(range(int(tot/2))):\n",
    "    abline(thetas[i][0], thetas[i][1], thetas[i][2], label_=None, c=lighten_color(colors_negative[int(tot/2)-i-1], 0.5), zorder=2)\n",
    "for i in range(int(tot/2)+1, tot):\n",
    "    abline(thetas[i][0], thetas[i][1], thetas[i][2], label_=None, c=lighten_color(colors_positive[i-int(tot/2)-1], 0.5), zorder=2)\n",
    "        \n",
    "abline(thetas[int(tot/2)][0], thetas[int(tot/2)][1], thetas[int(tot/2)][2], None, c=lighten_color('#e377c2', 0.5))\n",
    "\n",
    " \n",
    "\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], s=20, c=lighten_color('#bcbd22', 0.5), zorder=3, marker='X')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], s=20, c=lighten_color('#9467bd', 0.5), zorder=3, marker='<')\n",
    "#print(max_loss_id)\n",
    "    \n",
    "ax.tick_params(color='#dddddd')\n",
    "ax.spines['bottom'].set_color('#dddddd')\n",
    "ax.spines['top'].set_color('#dddddd')\n",
    "ax.spines['right'].set_color('#dddddd')\n",
    "ax.spines['left'].set_color('#dddddd')\n",
    "#plt.legend()\n",
    "\n",
    "plt.ylim(-6, 5)\n",
    "plt.xlim(-10, 4)\n",
    "plt.title(\"logistic regression\", fontsize=17)\n",
    "plt.xlabel(r\"$x_1$\", fontsize=17)\n",
    "plt.ylabel(r\"$x_2$\", fontsize=17)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"3-logistic_regression.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.concatenate((np.logspace(-1.5, 3, 30, endpoint=True), -1*np.logspace(-2, 1.5, 30, endpoint=True)))\n",
    "\n",
    "avg_=[]\n",
    "min_=[]\n",
    "max_=[]\n",
    "var_=[]\n",
    "for t in ts:\n",
    "    thetas = []\n",
    "    if t > 0 and t < 50:\n",
    "        lr_ = 0.01\n",
    "    else:\n",
    "        lr_ = 0.005\n",
    "    model = LogisticRegression(alpha=t, lr=lr_, num_iter=50000)\n",
    "    _, loss = model.fit(X, y)\n",
    "    print(\"t={}, max loss: {}, min loss: {}, avg loss: {}, variance: {}\".format(t, max(loss), min(loss), np.mean(loss), np.var(loss)))\n",
    "    \n",
    "    avg_.append(np.mean(loss))\n",
    "    max_.append(max(loss))\n",
    "    min_.append(min(loss))\n",
    "    var_.append(np.var(loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "\n",
    "colors_positive = pl.cm.Reds(np.logspace(-1, 0, 30))\n",
    "colors_negative = pl.cm.Blues(np.logspace(-1, 0.15, 30))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "i = 0\n",
    "for idx in range(len(avg_)):\n",
    "    if idx < 30:\n",
    "        col = colors_positive[i]\n",
    "    else:\n",
    "        col = colors_negative[i-30]\n",
    "    plt.scatter(max_[idx], avg_[idx], c=col)\n",
    "    i += 1\n",
    "    \n",
    "plt.ylabel(r\"average loss\", fontsize=15)\n",
    "plt.xlabel(\"max loss\", fontsize=15)\n",
    "plt.title(\"average vs. max loss tradeoffs\", fontsize=15)\n",
    "\n",
    "\n",
    "    \n",
    "ax.tick_params(color='#dddddd')\n",
    "ax.spines['bottom'].set_color('#dddddd')\n",
    "ax.spines['top'].set_color('#dddddd')\n",
    "ax.spines['right'].set_color('#dddddd')\n",
    "ax.spines['left'].set_color('#dddddd')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_tradeoff1.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "\n",
    "colors_positive = pl.cm.Reds(np.logspace(-1, 0, 30))\n",
    "colors_negative = pl.cm.Blues(np.logspace(-1, 0.4, 30))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "i = 0\n",
    "for idx in range(len(avg_)):\n",
    "    if idx < 30:\n",
    "        col = colors_positive[i]\n",
    "    else:\n",
    "        col = colors_negative[i-30]\n",
    "    print(min_[idx])\n",
    "    plt.scatter(min_[idx], avg_[idx], c=col)\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "plt.ylabel(r\"average loss\", fontsize=15)\n",
    "plt.xlabel(\"min loss\", fontsize=15)\n",
    "plt.title(\"average vs. min loss tradeoffs\", fontsize=15)\n",
    "\n",
    "plt.xscale('log')\n",
    "    \n",
    "ax.tick_params(color='#dddddd')\n",
    "ax.spines['bottom'].set_color('#dddddd')\n",
    "ax.spines['top'].set_color('#dddddd')\n",
    "ax.spines['right'].set_color('#dddddd')\n",
    "ax.spines['left'].set_color('#dddddd')\n",
    "plt.xlim(1e-7, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_tradeoff2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "\n",
    "colors_positive = pl.cm.Reds(np.logspace(-1, 0.1, 30))\n",
    "colors_negative = pl.cm.Blues(np.logspace(-1, 0.15, 30))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "\n",
    "for idx in range(len(avg_)):\n",
    "    if idx < 25:\n",
    "        col = colors_positive[idx]\n",
    "        plt.scatter(ts[idx], var_[idx], c=col)\n",
    "        \n",
    "    elif idx > 30 and idx < 55:\n",
    "        col = colors_negative[idx-30]\n",
    "        print(min_[idx])\n",
    "        plt.scatter(-1*ts[idx], var_[idx], c=col)\n",
    "\n",
    "    \n",
    "\n",
    "plt.ylabel(r\"variance\", fontsize=15)\n",
    "plt.xlabel(r\"$|t|$\", fontsize=15)\n",
    "plt.title(\"variance reduction\", fontsize=15)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "ax.tick_params(color='#dddddd')\n",
    "ax.spines['bottom'].set_color('#dddddd')\n",
    "ax.spines['top'].set_color('#dddddd')\n",
    "ax.spines['right'].set_color('#dddddd')\n",
    "ax.spines['left'].set_color('#dddddd')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylim(1e-4, 8)\n",
    "\n",
    "plt.savefig(\"loss_variance.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2D_LR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
